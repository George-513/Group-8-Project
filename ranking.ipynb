{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "        categories=categories,\n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "        shuffle=True)\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "def strip_nums(s):\n",
    "    return ''.join([i for i in s if not i.isdigit()])\n",
    "\n",
    "#all_docs=[nltk.tokenize.wordpunct_tokenize(strip_punctuation(e_book.data[i]).lower()) for i in range(n_docs)]\n",
    "all_docs=[nltk.tokenize.wordpunct_tokenize(strip_nums(strip_punctuation(e_book.data[i]).lower())) for i in range(n_docs)]\n",
    "\n",
    "\n",
    "bow=[[i for i in all_docs[j] if i not in stopwords] for j in range(n_docs)]\n",
    "\n",
    "\n",
    "bow=list(filter(None,bow))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tf-idf matrix :\n",
    "\n",
    "def unique(bow):\n",
    "    a = bow[0]\n",
    "    for i in range(1,len(bow)):\n",
    "        a = set(a).union(set(bow[i]))\n",
    "    return a\n",
    "\n",
    "\n",
    "wordset = unique(bow)\n",
    "worddict = [dict.fromkeys(wordset,0) for i in range(len(bow))]\n",
    "\n",
    "\n",
    "def term_document_matrix():\n",
    "    \n",
    "    for bow_i,worddict_i in zip(bow,worddict):\n",
    "        for word in bow_i:\n",
    "                    worddict_i[word]+=1\n",
    "    return pd.DataFrame(worddict)\n",
    "\n",
    "\n",
    "docterm = term_document_matrix()\n",
    "\n",
    "\n",
    "def term_freq(worddict,bow):\n",
    "    tfdict = {}\n",
    "    bowcount = len(bow)\n",
    "    for word,count in worddict.items():\n",
    "        \n",
    "        tfdict[word] = count/float(bowcount)\n",
    "    return tfdict\n",
    "\n",
    "\n",
    "tfbow =[term_freq(i,j) for i,j in zip(worddict,bow)]\n",
    "\n",
    "\n",
    "def idf(doclist):\n",
    "    idfdict={}\n",
    "    n = len(doclist)\n",
    "    \n",
    "    idfdict = dict.fromkeys(doclist[0].keys(),0)\n",
    "    for doc in doclist:\n",
    "        for word,val in doc.items():\n",
    "            if val>0:\n",
    "                idfdict[word]+=1\n",
    "    for word,val in idfdict.items():\n",
    "        idfdict[word]=math.log(n/float(val))\n",
    "    return idfdict\n",
    "\n",
    "\n",
    "idfs = idf(worddict) \n",
    "\n",
    "def tfidf(tfbow,idfs):\n",
    "    tfidf = {}\n",
    "    for word,val in tfbow.items():\n",
    "        tfidf[word]=val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidf = [tfidf(i,idfs) for i in tfbow]  \n",
    "\n",
    "X = pd.DataFrame(tfidf).T\n",
    "\n",
    "L,S,R=np.linalg.svd(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_pad_0(U,S,V,n):\n",
    "    t=list(S[:n])\n",
    "    for i in range(len(S)-n):\n",
    "        t.append(0) \n",
    "    A = np.diag(t)\n",
    "    newrow= [0]*len(S)\n",
    "    if len(U)>len(V):\n",
    "        for i in range(len(U)-len(S)):\n",
    "            A=np.vstack([A, newrow]) \n",
    "        return A\n",
    "    else:\n",
    "        for i in range(len(V)-len(S)):\n",
    "            A=np.vstack([A.T, newrow]).T  \n",
    "        return A\n",
    "        \n",
    "def reconstruct(u,s,v,n):\n",
    "    A=top_n_pad_0(u,s,v,n)\n",
    "    return np.round((u.dot(A)).dot(v),decimals=3)\n",
    "def frobenius(a,a2):\n",
    "    a =np.array(a)\n",
    "\n",
    "    return (np.sqrt(np.sum((a-a2)**2)))/np.sqrt(np.sum(a**2))\n",
    "\n",
    "def find_k():\n",
    "\n",
    "    for i in range(1,len(S)):\n",
    "        f=frobenius(X,reconstruct(L,S,R,i))\n",
    "        #print(f)\n",
    "        if f<0.38:\n",
    "            return i\n",
    "\n",
    "\n",
    "def search(q):\n",
    "    q=strip_punctuation(q)\n",
    "    q=q.lower().split(\" \")\n",
    "    terms = X.index\n",
    "    query=np.array([1 if i in q else 0 for i in terms])\n",
    "    if np.count_nonzero(query==1)==0:\n",
    "        print(\"Keywords don't match with documents\")\n",
    "    else:\n",
    "        score = query.dot(reconstruct(L,S,R,find_k()))\n",
    "        sort = sorted(zip(range(1,len(score)+1),score),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "        for i in range(len(sort)):\n",
    "            print(\"Document-{}\".format(sort[i][0]))\n",
    "        \n",
    "def print_concepts(n_concepts,n_keywords):\n",
    "    terms = X.index\n",
    "    for i,component in enumerate(L.T[:n_concepts]):\n",
    "        y = zip(terms,component)\n",
    "        z=sorted(y,key = lambda x:x[1],reverse=True)[:n_keywords]\n",
    "        print(\"concept %d:\"%(i+1))\n",
    "        for i in z:\n",
    "            print(i[0])\n",
    "            print(\" \")\n",
    "            \n",
    "def print_concept_docs(n_docs):\n",
    "    cols = X.columns\n",
    "    for i,component in enumerate(R):\n",
    "        y = zip(cols,component)\n",
    "        z=sorted(y,key = lambda x:x[1],reverse=True)[:n_docs]\n",
    "        print(\"Docs which contain most important concepts %d:\"%(i+1))\n",
    "        for i in z:\n",
    "            print(i[0])\n",
    "            print(\" \")\n",
    "            \n",
    "def plot_docs_3d():\n",
    "    y=R.T.dot(np.diag(S))\n",
    "    c = list(y[1])\n",
    "    d = list(y[2])\n",
    "    e = list(y[3])\n",
    "    fig = pylab.figure()\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "    sc = ax.scatter(c,d,e)\n",
    "    # for i, docs in enumerate(cols):\n",
    "    #     ax.text(c[i],d[i],e[i],  '%s' % (str(i)), size=20, zorder=1,color='k') \n",
    "    for x,y,z,i in zip(c,d,e,range(len(c))):\n",
    "        ax.text(x,y,z,i)\n",
    "    plt.xlabel(\"concept 1\")\n",
    "    plt.ylabel(\"concept 2\")\n",
    "    \n",
    "    \n",
    "        \n",
    "def plot_docs_2d():\n",
    "    y=R.T.dot(np.diag(S))\n",
    "    c = list(y[1])\n",
    "    d = list(y[2])\n",
    "\n",
    "    fig = pylab.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    sc = ax.scatter(c,d)\n",
    "\n",
    "        # for i, docs in enumerate(cols):\n",
    "        #     ax.text(c[i],d[i],e[i],  '%s' % (str(i)), size=20, zorder=1,color='k') \n",
    "    for x,y,i in zip(c,d,range(len(c))):\n",
    "        ax.text(x,y,i)\n",
    "        plt.xlabel(\"concept 1\")\n",
    "        plt.ylabel(\"concept 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
